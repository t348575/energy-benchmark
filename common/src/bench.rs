use core::fmt::Debug;
use std::{collections::HashMap, path::Path, pin::Pin, process::Stdio, time::Instant};

use downcast_rs::{Downcast, impl_downcast};
use dyn_clone::{DynClone, clone_trait_object};
use eyre::{Context, ContextCompat, Result, bail};
use flume::Sender;
use serde::{Deserialize, Serialize};
use tokio::{
    fs::File,
    io::{AsyncReadExt, AsyncWriteExt},
    process::{Child, Command},
    spawn,
    task::JoinHandle,
};
use tracing::debug;

use crate::{
    config::{Config, Settings},
    sensor::SensorRequest,
    util::read_until_prompt,
};

#[derive(Debug)]
pub struct CmdsResult {
    pub cmds: Vec<Cmd>,
    pub program: String,
}

#[typetag::serde(tag = "type")]
#[async_trait::async_trait]
pub trait Bench: Debug + DynClone + Downcast + Send + Sync {
    /// Benchmark program name (for identification purposes)
    fn name(&self) -> &'static str;
    /// Alias for [`Default`]
    fn default_bench() -> Box<dyn Bench>
    where
        Self: Sized + 'static;
    fn default_bench_args(&self) -> Box<dyn BenchArgs>;
    /// Return an estimate of how long the benchmark will take to run
    ///
    /// Returns:
    /// * An estimate in milliseconds of how long the benchmark will take to run
    fn runtime_estimate(&self) -> Result<u64>;
    /// Generates the commands to run the experiment with each argument combination to test
    ///
    /// Arguments:
    /// * `settings` - Settings from config file
    /// * `bench_args` - Arguments for the experiment
    /// * `name` - Name of the experiment run
    ///
    /// Returns:
    /// * 0: Program to run
    /// * 1: [`Vec<Cmd>`] generated commands for each configuration arrangment
    fn cmds(
        &self,
        settings: &Settings,
        bench_args: &dyn BenchArgs,
        name: &str,
    ) -> Result<CmdsResult>;
    /// Adds any additional arguments to arguments generated by [`Bench::cmds`] that require the final result directory: results, outputs etc.
    ///
    /// Arguments:
    /// * `_cmd` - cmd generated by [`Bench::cmds`]
    /// * `_final_results_dir` - final directory for results of run to be stored
    fn add_path_args(&self, _args: &mut Vec<String>, _final_results_dir: &Path) {}
    /// Add any additional environment variables
    ///
    /// Arguments:
    /// * `bench_args` - Arguments for the benchmark runner
    /// Returns:
    /// * HashMap of environment variables to add to the benchmark application
    fn add_env(&self, _bench_args: &dyn BenchArgs) -> Result<HashMap<String, String>> {
        Ok(HashMap::new())
    }
    /// Returns true if the benchmark requires a custom power state setter, i.e. do not use the NVMe CLI to set power states
    /// and expect the benchmark application to internally set the power state
    ///
    /// Returns:
    /// * True if the benchmark requires a custom power state setter
    fn requires_custom_power_state_setter(&self) -> bool {
        false
    }
    /// Check if results of an experiment run are OK (Check for deviations, etc.)
    ///
    /// Arguments:
    /// * `_results_dir` - Directory of local run (not particular iteration)
    /// * `_dirs` - List of directories to check (ie. each iteration of an experiment)
    ///
    /// Returns:
    /// * A list of index of directories (indexes corresponding to `dirs`) with results to be removed (and re-run)
    async fn check_results(&self, _results_dir: &Path, _dirs: &[String]) -> Result<Vec<usize>> {
        Ok(vec![])
    }
    /// Default initialization runner, override for custom logic
    ///
    /// Arguments:
    /// * `_data_dir` - Directory to store data in
    /// * `settings` - Settings from config file
    /// * `bench_args` - Arguments for the experiment
    /// * `_last_experiment` - The last experiment that was run
    async fn experiment_init(
        &self,
        _data_dir: &Path,
        _settings: &Settings,
        _bench_args: &dyn BenchArgs,
        _last_experiment: &Option<Box<dyn Bench>>,
    ) -> Result<()> {
        Ok(())
    }
    /// Default benchmark runner, override for custom logic
    ///
    /// Arguments:
    /// * `program` - Program to run
    /// * `args` - Arguments to run program with
    /// /// * `settings` - Settings from config file
    /// * `sensors` - Sensors that are available to record data
    /// * `final_results_dir` - Directory to store results in
    /// * `bench_obj` - Copy of the benchmark object (ie. self)
    /// * `config` - Config object
    /// * `last_experiment` - The last experiment that was run (helps prevent repetitive setup steps)
    async fn run(
        &self,
        program: &str,
        args: &[String],
        env: &HashMap<String, String>,
        settings: &Settings,
        sensors: &[Sender<SensorRequest>],
        final_results_dir: &Path,
        bench_obj: Box<dyn Bench>,
        _config: &Config,
        _last_experiment: &Option<Box<dyn Bench>>,
    ) -> Result<()> {
        let mut trace = None;
        if settings.should_trace.unwrap_or(false) {
            trace.replace(trace_nvme_calls(final_results_dir).await?);
        }

        let child = Command::new(program)
            .args(args)
            .stdout(Stdio::piped())
            .stderr(Stdio::piped())
            .envs(env)
            .spawn()
            .context("Running benchmark")?;
        debug!("Benchmark started");

        for sensor in sensors {
            sensor
                .send_async(SensorRequest::StartRecording {
                    dir: final_results_dir.to_path_buf(),
                    args: args.to_vec(),
                    program: program.to_string(),
                    bench: bench_obj.clone(),
                    pid: child.id().context("Could not get benchmark process id")?,
                })
                .await?;
        }
        debug!("Sensors started");

        let output = child.wait_with_output().await?;
        debug!("Benchmark done");
        if !output.status.success() {
            bail!(
                "Process exitied with {}, stdout: {}, stderr: {}",
                output.status.code().unwrap_or_default(),
                String::from_utf8(output.stdout).unwrap(),
                String::from_utf8(output.stderr).unwrap()
            );
        }

        for sensor in sensors {
            sensor.send_async(SensorRequest::StopRecording).await?;
        }
        debug!("Sensors stopped");

        if let Some(mut trace) = trace {
            trace.0.kill().await?;
            trace.1.await?;
        }
        Ok(())
    }
    /// Default post experiment runner, override for custom logic
    ///
    /// Arguments:
    /// * `_data_dir` - Directory to store data in
    /// * `_final_results_dir` - Directory of final results
    /// * `_settings` - Settings from config file
    /// * `_bench_args` - Arguments for the experiment
    async fn post_experiment(
        &self,
        _data_dir: &Path,
        _final_results_dir: &Path,
        _settings: &Settings,
        _bench_args: &dyn BenchArgs,
    ) -> Result<()> {
        Ok(())
    }
}
clone_trait_object!(Bench);
impl_downcast!(Bench);

#[typetag::serde(tag = "type")]
pub trait BenchArgs: Debug + DynClone + Downcast + Send + Sync {
    /// Benchmark program name, must be the same as [`Bench::name`]
    fn name(&self) -> &'static str;
}
clone_trait_object!(BenchArgs);
impl_downcast!(BenchArgs);

#[derive(Debug, Clone)]
pub struct Cmd {
    /// Arguments
    pub args: Vec<String>,
    /// Hash of arguments to use for the experiment folder name
    pub hash: String,
    /// An argument object, with only the arguments for this experiment
    pub bench_obj: Box<dyn Bench>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BenchmarkInfo {
    pub power_state: i32,
    pub iteration: usize,
    /// Name of the experiment, ie. [`crate::config::Config::bench_args::name`]
    pub name: String,
    pub hash: String,
    pub args: Box<dyn Bench>,
}

pub async fn trace_nvme_calls(trace_out_dir: &Path) -> Result<(Child, JoinHandle<()>, Instant)> {
    let trace_start_time = Instant::now();
    let mut bpftrace = Command::new("bpftrace")
        .arg("-e")
        .arg(include_str!("trace.bt"))
        .stdout(Stdio::piped())
        .spawn()
        .context("Running bpftrace")?;

    let mut bpf_file = File::create(trace_out_dir.join("trace.out")).await?;
    let mut bpftrace_stdout = bpftrace.stdout.take().context("Could not take stdout")?;
    let trace_job = spawn(async move {
        let mut stdout = Pin::new(&mut bpftrace_stdout);
        read_until_prompt(&mut stdout, "Attaching").await.unwrap();
        loop {
            let mut buf = [0u8; 1024];
            let n = stdout.read(&mut buf).await.unwrap();
            if n == 0 {
                break;
            }
            bpf_file.write_all(&buf[0..n]).await.unwrap();
        }
        drop(bpf_file);
    });
    Ok((bpftrace, trace_job, trace_start_time))
}
