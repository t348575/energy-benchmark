use core::fmt::Debug;
use std::{collections::HashMap, path::Path, pin::Pin, process::Stdio, time::Instant};

use downcast_rs::{Downcast, impl_downcast};
use dyn_clone::{DynClone, clone_trait_object};
use eyre::{Context, ContextCompat, Result, bail};
use flume::Sender;
use serde::{Deserialize, Serialize};
use tokio::{
    fs::File,
    io::{AsyncReadExt, AsyncWriteExt},
    process::{Child, Command},
    spawn,
    task::JoinHandle,
};
use tracing::debug;

use crate::{
    config::{Config, Settings},
    sensor::SensorRequest,
    util::read_until_prompt,
};

#[typetag::serde(tag = "type")]
#[async_trait::async_trait]
pub trait Bench: Debug + DynClone + Downcast + Send + Sync {
    /// Benchmark program name (for identification purposes)
    fn name(&self) -> &'static str;
    /// Alias for [`Default`]
    fn default_bench() -> Box<dyn Bench>
    where
        Self: Sized + 'static;
    fn default_bench_args(&self) -> Box<dyn BenchArgs>;
    /// Generates the commands to run the experiment with each argument combination to test
    ///
    /// Arguments:
    /// * `settings` - Settings from config file
    /// * `bench_args` - Arguments for the experiment
    /// * `name` - Name of the experiment run
    ///
    /// Returns:
    /// * 0: Program to run
    /// * 1: [`Vec<Cmd>`] generated commands for each configuration arrangment
    fn cmds(
        &self,
        settings: &Settings,
        bench_args: &dyn BenchArgs,
        name: &str,
    ) -> Result<(String, Vec<Cmd>)>;
    /// Adds any additional arguments to arguments generated by [`Bench::cmds`] that require the final result directory: results, outputs etc.
    ///
    /// Arguments:
    /// * `args` - previously generated args by [`Bench::cmds`]
    /// * `final_results_dir` - final directory for results of run to be stored
    fn add_path_args(&self, args: &mut Vec<String>, final_results_dir: &Path);
    /// Check if results of an experiment run are OK (Check for deviations, etc.)
    ///
    /// Arguments:
    /// * `results_dir` - Directory of local run (not particular iteration)
    /// * `dirs` - List of directories to check (ie. each iteration of an experiment)
    ///
    /// Returns:
    /// * A list of index of directories (indexes corresponding to `dirs`) with results to be removed (and re-run)
    async fn check_results(&self, results_dir: &Path, dirs: &[String]) -> Result<Vec<usize>>;
    /// Default benchmark runner, override for custom logic
    ///
    /// Arguments:
    /// * `program` - Program to run
    /// * `args` - Arguments to run program with
    /// /// * `settings` - Settings from config file
    /// * `sensors` - Sensors that are available to record data
    /// * `final_results_dir` - Directory to store results in
    /// * `bench_obj` - Copy of the benchmark object (ie. self)
    /// * `config` - Config object
    /// * `last_experiment` - The last experiment that was run (helps prevent repetitive setup steps)
    async fn run(
        &self,
        program: &str,
        args: &[String],
        settings: &Settings,
        sensors: &[Sender<SensorRequest>],
        final_results_dir: &Path,
        bench_obj: Box<dyn Bench>,
        _config: &Config,
        _last_experiment: &Option<Box<dyn Bench>>,
    ) -> Result<()> {
        let child = Command::new(program)
            .args(args)
            .envs(settings.env.as_ref().unwrap_or(&HashMap::new()))
            .stdout(Stdio::piped())
            .stderr(Stdio::piped())
            .spawn()
            .context("Running benchmark")?;
        debug!("Benchmark started");

        for sensor in sensors {
            sensor
                .send_async(SensorRequest::StartRecording {
                    dir: final_results_dir.to_path_buf(),
                    args: args.to_vec(),
                    program: program.to_string(),
                    bench: bench_obj.clone(),
                    pid: child.id().context("Could not get benchmark process id")?,
                })
                .await?;
        }
        debug!("Sensors started");

        let output = child.wait_with_output().await?;
        debug!("Benchmark done");
        if !output.status.success() {
            bail!(
                "Process exitied with {}, err: {}",
                output.status.code().unwrap_or_default(),
                String::from_utf8(output.stderr).unwrap()
            );
        }

        for sensor in sensors {
            sensor.send_async(SensorRequest::StopRecording).await?;
        }
        debug!("Sensors stopped");
        Ok(())
    }
    /// Return an estimate of how long the benchmark will take to run
    ///
    /// Returns:
    /// * An estimate in milliseconds of how long the benchmark will take to run
    fn runtime_estimate(&self) -> Result<u64>;
}
clone_trait_object!(Bench);
impl_downcast!(Bench);

#[typetag::serde(tag = "type")]
pub trait BenchArgs: Debug + DynClone + Downcast + Send + Sync {
    /// Benchmark program name, must be the same as [`Bench::name`]
    fn name(&self) -> &'static str;
}
clone_trait_object!(BenchArgs);
impl_downcast!(BenchArgs);

#[derive(Debug)]
pub struct Cmd {
    /// Arguments
    pub args: Vec<String>,
    /// Hash of arguments to use for the experiment folder name
    pub hash: String,
    /// An argument object, with only the arguments for this experiment
    pub arg_obj: Box<dyn Bench>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BenchmarkInfo {
    pub power_state: i32,
    pub iteration: usize,
    /// Name of the experiment, ie. [`crate::config::Config::bench_args::name`]
    pub name: String,
    pub hash: String,
    pub args: Box<dyn Bench>,
}

pub async fn trace_nvme_calls(
    trace_out_dir: &Path,
    settings: &Settings,
) -> Result<(Child, JoinHandle<()>, Instant)> {
    let trace_start_time = Instant::now();
    let mut bpftrace = Command::new("bpftrace")
        .arg("-e")
        .arg(include_str!("trace.bt"))
        .envs(settings.env.as_ref().unwrap_or(&HashMap::new()))
        .stdout(Stdio::piped())
        .spawn()
        .context("Running bpftrace")?;

    let mut bpf_file = File::create(trace_out_dir.join("trace.out")).await?;
    let mut bpftrace_stdout = bpftrace.stdout.take().context("Could not take stdout")?;
    let trace_job = spawn(async move {
        let mut stdout = Pin::new(&mut bpftrace_stdout);
        read_until_prompt(&mut stdout, "Attaching").await.unwrap();
        loop {
            let mut buf = [0u8; 1024];
            let n = stdout.read(&mut buf).await.unwrap();
            if n == 0 {
                break;
            }
            bpf_file.write(&buf[0..n]).await.unwrap();
        }
        drop(bpf_file);
    });
    Ok((bpftrace, trace_job, trace_start_time))
}
